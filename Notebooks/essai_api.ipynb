{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b64560",
   "metadata": {},
   "source": [
    "## Essai API ADEME AVEC PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12165128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres de cp 91\n",
      "[69790 69170 69250 69380 69009 69008 69006 69007 69005 69001 69004 69002\n",
      " 69003 69780 69360 69124 69580 69720 69960 69320 69800 69140 69330 69970\n",
      " 69730 69740 69150 69680 69510 69390 69910 69100 69640 69770 69400 69430\n",
      " 69820 69460 69200 69120 69420 69670 69890 69620 69240 69160 69220 69440\n",
      " 69490 69700 69560 69590 69270 69870 69210 69850 69930 69690 69550 69650\n",
      " 69830 69290 69230 69610 69110 69190 69450 69370 69280 69470 69480 69530\n",
      " 69310 69600 69350 69860 69760 69840 69540 69520 69340 69130 69570 69660\n",
      " 69115 69260 69410 69630 69300 69500 69126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ychen\\AppData\\Local\\Temp\\ipykernel_37648\\2248590103.py:10: DtypeWarning: Columns (9,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cp=pandas.read_csv(\"data/adresses-69.csv\",sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "import requests as req\n",
    "import json\n",
    "import pandas as pandas\n",
    "import numpy as np\n",
    "\n",
    "# URL de base de l'API\n",
    "base_url = \"https://data.ademe.fr/data-fair/api/v1/datasets/dpe03existant/lines\"\n",
    "\n",
    "# Fichier des codes postaux \n",
    "df_cp=pandas.read_csv(\"data/adresses-69.csv\",sep=\";\")\n",
    "cp69=df_cp[\"code_postal\"].unique()\n",
    "print(\"Nombres de cp\",len(cp69))\n",
    "print(cp69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# ---------- Paramètres ----------\n",
    "BASE_URL = \"https://data.ademe.fr/data-fair/api/v1/datasets/dpe03existant/lines\"\n",
    "YEARS = range(2020, 2025)                # 2020 → 2024\n",
    "PAGE_SIZE = 1000                         # 1000 conseillés pour réduire erreurs réseau\n",
    "OUT_CSV = \"donnees_dpe_rhone.csv\"\n",
    "\n",
    "# ---------- Session HTTP avec retries ----------\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,                # nb tentatives\n",
    "    connect=3, read=3,\n",
    "    backoff_factor=0.5,     # délai exponentiel: 0.5, 1, 2, ...\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"]\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# ---------- Chargement des CP (sans warning) ----------\n",
    "# force le type pour code_postal et low_memory=False pour éviter DtypeWarning\n",
    "df_cp = pd.read_csv(\"adresses-69.csv\", sep=\";\", dtype={\"code_postal\": str}, low_memory=False)\n",
    "cp69 = df_cp[\"code_postal\"].dropna().unique().tolist()\n",
    "print(\"Nombre de CP:\", len(cp69))\n",
    "\n",
    "# ---------- Utilitaire: fetch d'une page ----------\n",
    "def fetch_page(cp: str, year: int, offset: int):\n",
    "    borne1 = f\"{year}-01-01\"\n",
    "    borne2 = f\"{year}-12-31\"\n",
    "    params = {\n",
    "        \"size\": PAGE_SIZE,\n",
    "        \"from\": offset,  # pagination par offset\n",
    "        \"q\": cp,\n",
    "        \"q_fields\": \"code_postal_ban\",\n",
    "        \"qs\": f\"date_reception_dpe:{borne1} TO {borne2}\"\n",
    "        # Pas de 'select' => toutes les colonnes\n",
    "    }\n",
    "    # stream=False (par défaut) limite les erreurs chunked\n",
    "    r = session.get(BASE_URL, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    results = js.get(\"results\", [])\n",
    "    total = js.get(\"total\", 0)\n",
    "    return results, total\n",
    "\n",
    "# ---------- Ecriture CSV en append ----------\n",
    "def append_to_csv(df: pd.DataFrame, path: str, header_manager: dict):\n",
    "    # On fige le schéma la 1ère fois: toutes les colonnes de la première page\n",
    "    if header_manager.get(\"columns\") is None:\n",
    "        header_manager[\"columns\"] = list(df.columns)\n",
    "        df = df.reindex(columns=header_manager[\"columns\"])\n",
    "        df.to_csv(path, index=False, mode=\"w\", header=True)\n",
    "        header_manager[\"written_header\"] = True\n",
    "    else:\n",
    "        # on aligne les colonnes sur le schéma figé\n",
    "        df = df.reindex(columns=header_manager[\"columns\"])\n",
    "        df.to_csv(path, index=False, mode=\"a\", header=False)\n",
    "\n",
    "# ---------- Boucle principale ----------\n",
    "header_manager = {\"columns\": None, \"written_header\": False}\n",
    "total_rows_written = 0\n",
    "\n",
    "for cp in cp69:\n",
    "    for year in YEARS:\n",
    "        print(f\"\\n=== CP {cp} — Année {year} ===\")\n",
    "        offset = 0\n",
    "        page_idx = 1\n",
    "        total_for_query = None\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                results, total = fetch_page(cp, year, offset)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"[WARN] Requête échouée (offset {offset}). Nouvelle tentative...\")\n",
    "                time.sleep(1.5)\n",
    "                # une nouvelle tentative:\n",
    "                try:\n",
    "                    results, total = fetch_page(cp, year, offset)\n",
    "                except Exception as e2:\n",
    "                    print(f\"[ERROR] Abandon de la page offset={offset}: {e2}\")\n",
    "                    break  # on sort de la boucle de cette année/CP\n",
    "\n",
    "            if total_for_query is None:\n",
    "                total_for_query = total\n",
    "                print(f\"Total annoncé par l'API pour CP {cp} / {year}: {total_for_query}\")\n",
    "\n",
    "            if not results:\n",
    "                # plus de données\n",
    "                break\n",
    "\n",
    "            df_page = pd.DataFrame(results)\n",
    "\n",
    "            # écriture incrémentale\n",
    "            append_to_csv(df_page, OUT_CSV, header_manager)\n",
    "            total_rows_written += len(df_page)\n",
    "\n",
    "            print(f\"Page {page_idx} | lignes écrites: {len(df_page)} | total cumulé: {total_rows_written}\")\n",
    "\n",
    "            # pagination\n",
    "            offset += PAGE_SIZE\n",
    "            page_idx += 1\n",
    "\n",
    "            # micro pause pour éviter le throttling\n",
    "            time.sleep(0.2)\n",
    "\n",
    "print(f\"\\n✅ Terminé. Fichier écrit: {OUT_CSV} | Lignes totales: {total_rows_written}\")\n",
    "print(\"NB: Le schéma CSV est figé sur la 1ère page rencontrée (colonnes identiques ensuite).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
